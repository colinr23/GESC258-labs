[
["index.html", "GESC-258 Schedule Overview", " GESC-258 Schedule Colin Robertson 2021-03-19 Overview This is will provide week-to-week scheduling of course activities in GESC-258. "],
["introduction-and-research-design.html", "Week 1 Introduction and Research Design 1.1 What We Cover this Week 1.2 Readings 1.3 Lab", " Week 1 Introduction and Research Design Introduction to the course and basics of geographical research design. 1.1 What We Cover this Week This week will review the course syllabus and some basic research design concepts and concepts of statistical analysis. This will lay the groundwork for data analysis tools which we will cover in subsequent weeks. 1.2 Readings Chapter one in online textbook Key sections in readings: What are statistics? Descriptive vs. inferential statistics Levels of measurement Distributions Summation notation 1.3 Lab Attend your lab session on Friday possible to briefly meet your TA instructor and to get the course software installed on your computer. We will be using an open source software package called R and R-Studio for most of the work in this course. R-Studio is one of the most widely used software packages in data science, statistics and data analysis more generally. Many employers in the environmental field now greatly value skills in R - so even though the learning curve can be a little steeper, it is well worth the effort to learn this approach to data analysis. As a supplement to the lab materials, I highly recommend using YaRrr! The Pirate’s Guide to R - which has lots of great information and resources for learning R. You can read chapter 2 for how to get it installed on your machine. The TA will verify everyone has a working installation of R on their machine in lab on Friday. "],
["w2title.html", "Week 2 Probability Distributions - Normal 2.1 What we cover this week 2.2 Readings 2.3 Lab", " Week 2 Probability Distributions - Normal This week we review the Normal distribution as a way into thinking about probability and comparing observations to a distribution. 2.1 What we cover this week The Normal distribution Z-scores Calculating probability from z-scores Understanding how and why the Normal distributions is important Limitations and situations where the use of the Normal distribution is inappropriate 2.2 Readings Chapter VI in online textbook Key sections in readings: Introduction History Areas of Normal Distribution Varieties of Normal Distribution Standard Normal 2.3 Lab This lab is due on the Dropbox on MyLearningSpace on Friday January 29th Our lab this week will examine trees in a coastal forest in British Columbia. In particular we have a small dataset of tree circumfrences which we want to analyze. These data were collected recently by me and my trusty field assistant, which you can review in its entirety below. You can see how time consuming data collection can be! While tree circumference is easy to measure and tells us something about tree size and age, it is more customary to work with tree diameters. A typical measure used to measure tree is diameter-at-breast-height (DBH) which is measured 4.5 feet from the ground using a DBH tape. Since we did not have one of these specialized DBH tapes, we can get started by converting our measures of circumference to diameter using the relationship between these two quantities which is \\(diameter=Circumference/\\pi\\) recalling that \\(\\pi\\) is ratio of a circle’s circumference to its diameter which we can use 3.14159 as a value. So assuming we have a circumference of 214 cm, the diameter would be \\(214/3.14159=68.11837\\) which in our dataset our expressed in units of centimeters. 2.3.1 Tree Circumference Data The data we collected in the video were as follows: 272 272 236 154 256 156 143 269 205 175 In order to bring the data into R, we need to assign these numbers to a data frame which is like a table, or to a vector which is just a bunch of numbers stored in order. A data frame can be made up of multiple vectors (as individual columns in a table). We will make a vector, since we do not have anything other than tree circumference to store. If we had collected information on tree species, condition, site characteristics, etc. we would want to store the data as a table, so we would create a data frame. x = c(272,272,236,154,256,156,143,269,205,175) What is happening in the command line above? First of all we are making an assignment using the \\(=\\) operator. This means we are assigning whatever is on the right side of the \\(=\\) to what is on the left side. In this case we are creating a vector of numbers representing our tree dataset to a variable called \\(x\\). The function c is being used which all it does is take some values and create a vector from them. Before going further, as we noted above we want to convert these measures of circumference to diameters. R has a special number for \\(\\pi\\) which we can use by just typing pi in the console. Try it: pi ## [1] 3.141593 So if we wanted to divide all of our values by \\(\\pi\\) we could just type the following in the console: x/pi ## [1] 86.58029 86.58029 75.12113 49.01972 81.48733 49.65634 45.51831 85.62536 ## [9] 65.25353 55.70423 which is good, but this just displays them to the screen. In order to do something with these we want to store them in a new variable. Note that we can use any name we want for variables. We will create a new one called dbh: dbh = x/pi Now we have a new vector which has our data properly represented in values of dbh which we measured in the field. We can use this vector in subsequent work below. 2.3.2 Recalling measures of central tendency and dispersion Say we want to calcuate some measures of central tendency to give us an idea what we might expect to find for a randomly selected tree in this forest. We could calcualte the mean, median, and mode. The mean and median are useful, but the mode doesn’t make much sense in this context, so we will focus on the mean and median. Recall the difference - the mean just the sum of values divided by the number of observations - which is expressed in math terms as \\(\\bar{x}=\\frac{\\sum\\limits_{i = 1}^n{x_i}}{n}\\) which if this looks foreign to you not to worry. This is saying the mean, denoted by \\(\\bar{x}\\) is equal to the sum of the observation values (\\(x_i\\) in the above) divided by \\(n\\) which is what we used to denote the number of observations or the sample size. The sample size is a super important quantity in everything we’ll do. It is always simply a count of the number of measurements, but it can tell us a lot about how much certainty we should have in our data and statistics. For example, in the above, we only measured 10 trees, out of thousands in that forest. We may not be able totally rely on measures calculated from such a small sample (especially since the selection of trees was far from random!). So we know at this point we have the data we want stored in dbh and we know what we want to do with it - calculate the mean. We can use R just like a calculator with a few tricks thrown in to save us some time. One is the sum function. We can for example, sum up the dbh values: sum(dbh) ## [1] 680.5465 which gets us most of the way there, if we divide by our sample size, we will get the mean. We can do this in one step: sum(dbh/10) ## [1] 68.05465 which tells us that 68.0546537 is the mean dbh in the sample of trees. We can also use the built in mean function: mean(dbh) ## [1] 68.05465 We can also get the median with a simple function: median(dbh) ## [1] 70.18733 If we wanted to compute the standard deviation, which in math terms looks like \\(\\sigma = \\sqrt {\\frac{\\sum\\limits_{i = 1}^n {\\left( {x_i - \\bar x} \\right)^2 }}{n}}\\) which is a bit more complicated than the mean, but if we work from the inside out, we are summing the squared deviations \\(\\left( {x_i - \\bar x} \\right)^2\\) from the mean, dividing that sum by the sample size \\(n\\) and then square rooting the whole thing to get us out of squared terms. We can use the function sd in R to calculate the standard deviation: sd(dbh) ## [1] 16.97905 which we could also calculate ‘manually’ like: dbh - mean(dbh) ## [1] 18.525635 18.525635 7.066479 -19.034931 13.432677 -18.398311 ## [7] -22.536340 17.570706 -2.801127 -12.350424 (dbh - mean(dbh))^2 ## [1] 343.199166 343.199166 49.935132 362.328606 180.436817 338.497863 ## [7] 507.886618 308.729699 7.846312 152.532963 sum((dbh - mean(dbh))^2) ## [1] 2594.592 sum((dbh - mean(dbh))^2)/10 ## [1] 259.4592 sqrt(sum((dbh - mean(dbh))^2)/10) ## [1] 16.10774 You may be asking yourself (hopefully) - hey our manual method doesn’t match the output we got from the sd function? The reason is because there are actually two ways to calculate standard deviation - one way for sample data, and another way for population data. The good news is that the difference is very small - basically when we use sample data we have to reduce our sample size by one in order to get what is called an unbiased estimate. We will go into more detail about this later, but for now, recognize that the sd function defaults to the sample standard deviation. Since we are working with sample data, we should use the sample formula, and adjust our manual method: sqrt(sum((dbh - mean(dbh))^2)/9) ## [1] 16.97905 now they match! sd(dbh) ## [1] 16.97905 2.3.3 The Normal Distribution Lets pretend that Clementine and I spent weeks out in that forest dutifully sampling trees so that we had a sample size of 1500! I can’t see that having gone well. But luckily we can create a dataset using simulation pretty easily, and we will based it on our actual sample mean and standard deviation. We will use a function called rnorm which basically will generate random samples from a given Normal distribution, we just have to supply the mean, standard deviation, and sample size. sim_dbh &lt;- rnorm(n=1500, mean=mean(x), sd=sd(x)) We don’t want to look at all the values on the screen because there are too many. We can look at the first few, the last few, and then count the number of observations as follows: head(sim_dbh) ## [1] 193.2603 265.5825 315.8979 216.8749 215.1430 255.1160 tail(sim_dbh) ## [1] 200.8646 235.3436 243.4558 118.5888 254.8605 133.7334 length(sim_dbh) ## [1] 1500 because these are randomly simulated values, yours may differ a bit from what you see here, but there should be 1500 values. We can look at the distribution of values by plotting the histogram: hist(sim_dbh, xlab = &quot;Simulated DBH(cm) Values&quot;, main=&quot;&quot;) We can also create a line showing the theoretical distribution that would be the population standard deviation: x = seq(0,200,.1) y = dnorm(x, mean=mean(dbh), sd=sd(dbh)) plot(x,y, type=&quot;l&quot;, main=&quot;Normal Distribution&quot;) The key point here is that from this smoothly varying distribution, we can calculate probabilities. Say we sampled a tree with a dbh of 100, we can ask the question - what is the probability of finding a tree with a dbh of 100 or greater from this distribution, that is akin to finding the area under the curve for everything to the right of the 100 value on the x-axis. We will use a special probability function in R to get this, it is called pnorm pnorm(q=100, mean = mean(dbh), sd=sd(dbh), lower.tail = FALSE) ## [1] 0.02995489 which tells us that the probability of sampling a tree with a dbh value of 100 or greater is 0.03 - so pretty unlikely. 2.3.4 Using z-scores instead of raw data to more easily find probabilities from a normal distribution You may have covered z-scores in GESC-254, but as a refresher, it is simply a score we can calculate for every observation which standardizes the data to a standard normal distribution. This means it eliminates the units of the data (i.e., we’re no longer in the centimeters we measured) - but it makes scores comparable across datasets and easier to quickly interpret. The z-score formula is \\(z_i = \\frac{x_i - \\bar{x}}{\\sigma}\\) which is just the observation, minus the mean, divided by the standard deviation. We’ll go back to our smaller dataset to illustrate: (dbh - mean(dbh)) / sd(dbh) ## [1] 1.0910881 1.0910881 0.4161882 -1.1210836 0.7911326 -1.0835892 ## [7] -1.3273030 1.0348464 -0.1649755 -0.7273920 z-scores have the properties that values less than zero are for observations below the mean, values greater than zero are for observations above the mean, and values of zero correspond to the mean (which are rare). Moving back to our hypothetical tree with a dbh of 100, lets calculate its z-score: (100 - mean(dbh)) / sd(dbh) ## [1] 1.881457 which gives us a z-score of 1.881457 If we checked the probability value associated with this z-score we would see that it matched the one we got above, we just no longer have to specify the mean and sd because this is referenced to a standard normal distribution pnorm(1.881457, lower.tail=FALSE) ## [1] 0.02995489 2.3.5 Assignment Calculate the probability of finding a tree with a dbh of 90 or greater based on the sample mean and sample standard deviation above. What is the z-score associated with a dbh of 90 cm? Include commands used to generate the answer. (out of 3) Repeat question 1 for a distribution with a mean of 75 and a standard deviation of 30. Include a sentence intepreting what the difference in the two answers means. (out of 4) Create a new dataset with a mean and standard deviation of your choosing and a sample size of 100. Plot the histogram being sure to label axes appropriately. (out of 3) Do you think the sample of trees we collected was representative of the wider forest? What would be some potential issues with inferring characteristics of the forest from this dataset? (out of 5) What other types of data could we have collected? What would be of greatest value if we were aiming to identify the conservation value of the forest to a community group? (out of 5) "],
["w3title.html", "Week 3 Probability Distributions II - Others 3.1 What we cover this week 3.2 Readings 3.3 Lab", " Week 3 Probability Distributions II - Others This week we review distributions other than the Normal. While the Normal distribution is super useful, sometimes the data we are collecting are not suitable for it. If we are counting rare events for example - the counts will be low and left skewed toward zero likely. A Normal distribution in this case would have some of its tail below zero (which doesn’t make sense for count data), as well, counting variables are discrete random variables - they can only take on values of whole integer numbers (i.e., no half counts!). Luckily, the way we work with discrete distributions in R is very similar to how we work with the Normal distribution. We just have to remember that different distributions have different defining parameters. 3.1 What we cover this week The Binomial distribution The Poisson distribution The discrete and continuous uniform distributions Understanding when to use a given distribution 3.2 Readings Chapter V in online textbook Key sections to focus on in the readings: Introduction Basic Concepts Permutations and Combinations Binomial Distribution Poisson Distribution 3.3 Lab This lab is due on the Dropbox on MyLearningSpace on Friday February 5th Our lab this week will examine a specific process of rock weathering in sandstone found in coastal rocky areas in coastal British Columbia. This weathering process produces characteristic ‘honeycomb’ patterns in sandstone which can easily be found in coastal sandstone around the world. These features are called by several names, such as honeycomb weathering, cavernous weathering, and ‘tafoni’. In coastal salt-rich environments such as where we will be exploring, we will investigate how this unique pattern of rock weathering forms and how we can characterize their patterns using probability distributions. But most importantly let’s take a look at what we’re talking about: Figure 3.1: Honeycomb weathering on the coast of Gabriola Island, British Columbia. Image credit: Colin Robertson These rock features are pretty cool to see and very distinctive. Here are a few more examples taken from the same site: all photos were taken at approximately the same vertical distance so the scale is approximately equal across these images. Figure 3.2: More examples of honeycomb weathering on the coast of Gabriola Island, British Columbia. Image credit: Colin Robertson 3.3.1 How does honeycomb patterns in sandstone form? There are a variety of mechanisms that produce these regular weathering patterns in sandstone, but in salt-rich coastal environments such as that here, the primary process is a result of what is called salt crystallization. In short, rockfaces exposed to sunlight have salt-rich water drawn up to the surface where the salt crystallizes as the water evaporates. The salt crystallization exerts pressure on the surround rock to create tiny factures which become the pits we recognize in the pattern above. As a result of this process, we therefore expect to see these interesting features where sandstone has direct sunlight exposure, such as on south-facing sections of coastline. We can investigate the density of the weathering process. We will take one image as our study dataset, then count the number of pits in different sections of the image. Unlike in Lab 1 where we sampled trees from a forest, here we are taking one small section of coastline and enumerating the density of weathering pits for the entire small section. This is still a sample - and our population is the sandstone surfaces undergoing this process in general. Think about what potential issues there may be in making inferences from the data to the population in this case. 3.3.2 Study Image The image below we will use as our dataset.We have placed a regular 6 x 6 grid over the image which will serve as our sampling units. Then we will count the number of pits in each cell and generate a dataset of counts. Since this will be a dataset of counts, we will consider this a discrete random variable. Figure 3.3: Study image dataset featuring honeycomb weathering on the coast of Gabriola Island, British Columbia. Image credit: Colin Robertson In order to count the number of pits per cell, we have to make some decisions; namely what counts as a pit and how do we associate pits with cells since in many cases the hole crosses cell boundaries. This is the sort of research decision we have to make carefully, and then later consider how it impacts our analysis. We will use the following criteria. A pit will be defined based on having visible walls (i.e., we can discern the edges/sides of the pit) and further, we will link it with a cell based on the centroid of the pit. Since this step requires some interpretation which may vary - I have done this for you digitizing our dataset of pit centroids, as below: Figure 3.4: Study image with pits digitized. Image credit: Colin Robertson Your first job is to create a dataset by counting the number of pits in each cell. I have done the first row for you to give us a working dataset as an example, but when you do the assignment you will have to get the counts for all cells - you should end up with a datset with 36 observations, each of which is a count. Figure 3.5: First row of cells with number of pits counted. Image credit: Colin Robertson So to get started our dataset would be created in R as follows: pits &lt;- c(4,7,9,4,5,0) recall why we are treating this as a discrete random variable. Based on our criteria and the nature of the dataset, we have counts - which cannot take on fractional values. This was a decision which impacts how we treat the data. An alternative way to analyze the data would be to measure the fraction of each cell covered by a pit, which we could then treat as a continuous random variable. 3.3.3 Analyzing Count Data The first thing we can do if we want to estimate the parameters of the Poisson distribution with our dataset, is to calculate the sample statistic estimate of the distributions parameters. In Lab 1 we did this by calculating the sample mean and sample standard deviation. The Poisson distribution has a probability mass function as follows: \\(P(X = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\) and the only important thing for you to know is that there is one unknown quantity required to define it (i.e., one parameter) - which is the mean number of events per unit of observation - which in our case is the cells which we counted pits in. The sample estimate of \\(\\lambda\\) is therefore simply: mean(pits) ## [1] 4.833333 so we know that is the average number of pits, but we are not sure what the overall distribution would look like. Again we can simulate some new data using our estimate of \\(\\lambda\\) that we calculated from the data to see simpits = rpois(n=1000, lambda = mean(pits)) hist(simpits, xlab = &quot;Simulated Pit Count Values&quot;, main=&quot;&quot;) which again might vary a bit for you, but should be a distribution centered on around 5 and tailing off around 12 or 14. Lets look at this and notice a few details; the lowest value is zero the higher values quickly trail off the distribution shape is almost normal the Poisson is useful for what we call rare events because of these properties of being bounded by zero on the lower end. If you have count data that has very high counts, the difference between the Poisson and the Normal is negligible and you can just use a Normal distribution (this is called a Normal approximation). Lets look at the Poisson functions. The function dpois gives the probability of x number of events observed given the expected number specified by \\(\\lambda\\). Whereas the function ppois gives a cumulative probability - similar to what we did in the Normal distribution, where we have to specify the upper or lower tail (i.e., equal or less than vs. equal or greater than). Say we wanted to calculate the probability of observing 7 pits in a cell given our mean number per cell of 4.8333333 - we could of course work through the formula on a calculator or using R math functions - and it would look like this: #compare this to the probability function noted above exp(-mean(pits))*mean(pits)^7/factorial(7) ## [1] 0.09732103 but it is much easier to use the built in function dpois to get the probability: dpois(x = 7, lambda = mean(pits)) ## [1] 0.09732103 we get the same answer but using dpois is a lot cleaner and easier. We can also plot the probability for each of the observed counts in our dataset. We just have to supply the whole vector of counts to the same dpois function plot(pits, dpois(x = pits, lambda = mean(pits)), pch=20, cex=2, xlab = &quot;Pit Count&quot;, ylab = &quot;Probability&quot;) So now we have the ability to calculate probabilities from the Poisson distribution in R. How do we answer meaningful questions with these tools? While the process of salt crystallization is the dominant one creating the weathering pits, we may want to know how pits form. For example we could ask how many can form before adjacent pit edges dissolve into one larger pit. What do you think the maximum number of pits per unit area is? Where we see large pits form, are these the gradual growth of single pits or the amalgamation of several into one larger one? The other thing to think about is where we have large pits, the count in the cell is likely going to be lower. 3.3.4 Assignment Calculate the probability of observing a cell with 15 pits based on the sample intensity (\\(\\lambda\\) computed from all cells in the image). Write a sentence interpreting what this means. Include commands used to generate the answer. (out of 2) What is the probability of observing a cell with between 3 and 5 pits (\\(\\lambda\\) computed from all cells in the image)? Write a sentence interpreting what this means. Include commands used to generate the answer. (out of 3) There are two important assumptions to using the Poisson distribution we should consider. The probability that an event will occur within a given unit must be the same for all units (i.e. the underlying process governing the phenomenon must be invariant) The number of events occurring per unit must be independent of the number of events occurring in other units (no interactions/dependencies). Write short paragraph (200-300 words) explaining why or why not these assumptions are met in this analysis of the weathering pits dataset. (out of 5) Explore the assumption of independence by calculating the probability of each observed count and noting where on the image any counts with a probability less than 0.10 occur. Comment on whether these are distributed randomly over the image or clumped in specific parts of the image and what this means for the independence assumption. Include an image showing which cells have an unusual (i.e., p &lt; 0.10) number of pits. (out of 5) Hint: to answer 4 you can take a screenshot from the lab and use MSPaint (Windows), Photos (Mac) or another graphics program to identify which cells have unusual counts. "],
["w4title.html", "Week 4 Sampling Distributions and Confidence Intervals 4.1 What we cover this week 4.2 Readings 4.3 Lab", " Week 4 Sampling Distributions and Confidence Intervals This week we review the Normal distribution as a way into thinking about probability and comparing observations to a distribution. 4.1 What we cover this week Sampling distributions Sampling error Confidence intervals and estimation 4.2 Readings Chapter IX in online textbook Key sections: Introduction Sampling Distribution of the Mean Sampling Distribution of p Statistical Literacy Chapter X in online textbook Key sections: Introduction Degrees of Freedom Confidence Intervals Confidence Intervals Intro Confidence Intervals for Mean Confidence Intervals for proportion 4.3 Lab This lab is due in the Dropbox on MyLearningSpace on Friday February 26th Our third lab will cover sampling distributions and confidence intervals. To learn about these we are going to explore a new dataset which was painstakingly collected on a beach after a large storm. Our objective here is to try to estimate the average size of driftwood on the beach. Driftwood on beaches in coastal British Columbia is very common, and most are escaped logs from logging booms - while the odd one naturally uprooted shows up as well. Tracking how driftwood accumulates in different sections of beach is a fascinating combination of where logging takes place, how logs are transported, ocean currents and storm activity, etc. There is in fact a whole job category devoted to capturing escaped logs and selling them back to the logging companies - an occupation made famous by the epic Canadian 1980s television show - The Beachcombers - set in Gibson’s B.C. We set out to record the length of a sample of beach on the morning after a large storm. Our intended goal was to record the length of every notable log in the section of beach being sampled, but data collection had to be cut early due to an unruly field assistant. In all we collected the following 8 measurements: x = c(653, 646, 654, 153, 305, 1200, 1193, 172) from which we can obtain a sample mean and sample standard deviation; as well as the summary command to see more descriptive statistics; mean(x) ## [1] 622 sd(x) ## [1] 411.5538 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 153.0 271.8 649.5 622.0 788.8 1200.0 which shows that there is a lot of variability in the lengths. We also have a tiny sample size which is not great. Lets look at the frequency distribution of our sample: hist(x, xlab=&quot;Driftwood Length (cm)&quot;, main=&quot;&quot;) which shows the data is not normally distributed. However - we know we can use the sampling distribution of the mean to calculate the standard error of the mean even for non-normal data. Generally we want a larger sample size (n &gt;= 30), but we will use what we have to illustrate. Recall that for the sampling distribution of \\(\\bar{x}\\) the mean of sample means \\(\\mu_{\\bar{x}}\\) is equal to \\(\\mu\\) and that \\(\\sigma_{\\bar{x}}\\) is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\). Now, because we do not know what \\(\\sigma\\) is - we have to use our sample estimate of it, which is \\(s\\) which we calculate using the sd function - sd(x) ## [1] 411.5538 so we could calculate \\(\\frac{\\sigma}{\\sqrt{n}}\\) using sd(x) / sqrt(8) ## [1] 145.5063 4.3.1 Sampling Distribution of the Mean The sampling distribution for our mean is estimated as coming from a normal distribution with \\(\\mu = \\mu_{\\bar{x}}\\) and \\(\\sigma_{\\bar{x}}\\) = \\(\\frac{\\sigma}{\\sqrt{n}}\\). Let’s pretend we knew the actual values of \\(\\mu\\) and \\(\\sigma\\) and use the following two values for the population, a \\(\\mu\\) of 860 and a \\(\\sigma\\) of 270. The z-score of the sample mean is: \\(z = \\frac{\\bar{x}-\\mu}{\\sigma_{\\bar{x}}}\\) so the question is how unusual is the mean we observed in our small sample? We can use the normal distribution as we have previously using the pnorm function. Let’s calculate the z-score: (mean(x) - 860) / (270/sqrt(8)) ## [1] -2.493206 right away we should notice a couple of things with this z-score; The z-score is below 0 and therefore \\(\\bar{x} &lt; \\mu\\) The z-score is below -2 so this is a pretty unlikely outcome The probability of such a z-score or lower is pnorm(-2.493206, lower.tail = TRUE) ## [1] 0.006329769 Now what would have happened if everything else was the same, but our sample size was only n = 2? Then we would be looking at: zscore = (mean(x) - 860) / (270/sqrt(2)) pnorm(zscore, lower.tail = TRUE) ## [1] 0.1062715 now this seems much more likely even though the sample mean stayed the same - this is because a sample statistic (or estimate of \\(\\mu\\) using \\(\\bar{x}\\)) is highly uncertain if we only measured 2 logs. 4.3.2 Confidence Intervals A better way to capture the uncertainty in our sample statistic is with a confidence interval. Instead of calculating a z-score and finding a probability we invert that process - so we set the probability - which is our confidence level, then we find the associated value of z-score for this probability, then we find the interval of values around \\(\\bar{x}\\). A generic formula for a confidence interval is: point estimate +/- critical value * standard error Now let’s go through the steps above: Set the probability. There are common levels which we use such as 95% or 99% or 90%. We will use a 95% confidence level. Now we need to find a z-score associated with 95% probability. We do this by taking the inverse so 1-0.95. We could then use qnorm to find the z-score for 0.05. But this would be wrong because qnorm gives us one-tailed probabilities and we really need two-tailed (i.e., 0.05 split into the upper and lower tails), so we would use qnorm(0.05/2) which would give us a lower z-score associated with .025 probability. This is what we use as the critical value of the confidence interval. Now to find the interval in R we just need the standard error - which we already know is \\(\\frac{\\sigma}{\\sqrt{n}}\\) So it would look something like: z_crit = qnorm(0.05/2, lower.tail = FALSE) lower = mean(x) - (z_crit * (270/sqrt(8))) upper = mean(x) + (z_crit * (270/sqrt(8))) plot(x=c(lower, upper), y=c(1, 1), type=&quot;l&quot;, xlim = c(min(x),max(x)), xlab=&quot;Driftwood Length (cm)&quot;, ylab=&quot;&quot;, col = &quot;Black&quot;) points(x = mean(x), y = 1, pch=20, cex=3) #we could add another to it if we wanted z_crit = qnorm(0.10/2, lower.tail = FALSE) lower = mean(x) - (z_crit * (270/sqrt(8))) upper = mean(x) + (z_crit * (270/sqrt(8))) lines(x = c(lower, upper), y=c(1.2, 1.2), col=&quot;red&quot;) points(x = mean(x), y = 1.2, pch=20, cex=3) So there is a lot of extra commands here you can ignore. The key thing being that what do we notice about the second (red) interval we created? It is narrower. This is because we set the probability value to be lower (i.e., we have lower confidence) - which basically means - we can be less sure (only 90% instead of 95%) about a narrower interval - for the same data. If you have made it this far, you may be are forgetting what the data actually represent. Recall these are driftwood lengths measured on a section of beach in B.C. We tried to sample a lot more but with no luck. The data here is used only for the purposes of illustration - but since I recorded our attempts at data collection - here is a video of our beach sampling - with some background music to set the scene. Watching this will not help with the lab but may be entertaining! The above procedure for a confidence interval works for when we have a normally distributed test statistic, that is why we use a z-score as the critical value. In cases where we have a small sample size we can actually use another distribution - the t distribution - as the critical value. In this way we can capture the greater uncertainty associated with very small sample sizes. We can use the qt function to get critical values from the t-distribution. Take a look at the help to see how to set the degrees of freedom. 4.3.3 Assignment Because our dataset was terrible this week and this is a lab which we have three weeks to complete, we are doing things a little differently. It is a challenge to create datasets that are of interest to all students so instead for this lab we are going to get you to find your own. This can be as simple or as complex as you want to make it. The key point is that you collect/create the dataset yourself (i.e., this is not taken from an example elsewhere). Try to get a sample size of at least 30. If collecting this dataset requires you going outside and measuring something - even better. Or this can be something related to sports, music, whatever interests you. Use Piazza to ask questions about suitable datasets if you are not sure. You could for example check the temperature at environment canada on this day for the past 30 years. Or you could check with goals against average for Toronto Maple Leafs goalies for the past 30 years. It is up to you - but the dataset needs to be unique to you and you will have to explain where it comes from in your write up. Write a paragraph describing your dataset. Include how you acquired it, why you selected it, what the variable of interest is, whether there is any measurement error, sources of sampling error, bias, etc. Also include a histogram and a table with basic summmary statistics describing your dataset. The histogram and table should have descriptive captions. Include your dataset with your R commands. (out of 10) Create 90%, 95% and 99% confidence intervals for your data (using a sample mean). Plot these on graph. Include a sentence describing what these mean and commands used to generate the answer. (out of 10) Repeat everything for question 2 but this time use a t-critical value (i.e., you will have to use the qt function to find the critical value). You can use the same or a different variable (than that used in question 2) to analyze - use sample to create a random sample of size n = 20 if you have more than 20 observations in your dataset. (out of 10) Conduct your own analysis of the dataset. This could include calculating z-scores to find unusual values or groups of unusual values and any plots you want to explore. Write a paragrph commenting on the results and interpreting your findings. (out of 10) "],
["hypothesis-testing-concepts.html", "Week 5 Hypothesis Testing Concepts 5.1 What we cover this week 5.2 Readings 5.3 Lab", " Week 5 Hypothesis Testing Concepts This week we introduce hypothesis testing - a very important concept and methodological framework. 5.1 What we cover this week The null and alternative hypotheses P-values Significance level Test statistics Type I and Type II errors One and two-tailed tests 5.2 Readings Chapter XI in online textbook Key sections: Introduction Significance Testing Type I and Type II Errors One- and Two-Tailed Tests Interpreting Significant Results Interpreting Non-Significant Results 5.3 Lab This week’s lab is a little different. Instead of calculating quantities from data (which we will do next week, don’t worry), we are going to focus on intepreting existing research results, to get a feel for how these tools are actually used. In particular, we are going to focus on a t-test for a sample mean, and hypothesis tests about sample means. Your first objective will be to find a scientific paper that interests you. Go to google scholar and to search enter “t-test” AND “something that you want to read research about”. Make sure you include the quotation marks. For example I will use “t-test” AND “polar bears” just to get started. This brings up this page of search results. Now you probably do not want to select the first article you see. You want to select an article based on the following criteria: is it actually about the topic your are intersted in is a t-test for a mean used in the analysis can you understand the paper do you have access to the full text of the paper You may have to go in through the library website to gain access to a paper, othertimes you will see a link from Google Scholar directly to a PDF of the paper. Your goal is to select a paper of interest which you will then answer questions (below) about for this week’s lab assignment. On the second page of results, I found the following paper which I thought met the criteria above. T-tests were used here to compare the mean litter size among samples of polar bear cubs in two regions (Hopen Island vs. the rest of Svalbarad), among other things. 5.3.1 Assignment Once you find a paper you want to work with for this lab, answer the following questions below: Write a paragraph summarizing your paper. What were the research objectives, what data were analyzed, what did the results show? (out of 5) What specific hypothesis was tested using a t-test in this paper? Was it upper, lower, or two-tailed? What was the result? Include a screenshot of the result from the paper. (out of 5) What limitations are there in the study? What other types of analysis or data could the researchers have used to address their objective(s)? (out of 4) What does the p-value reported in the paper mean? Include a screenshot of the result from the paper. (out of 3) What was the significance level of the hypothesis test in the paper? What does it mean? Include a screenshot of the result from the paper. (out of 3) "],
["hypothesis-testing-i.html", "Week 6 Hypothesis Testing I 6.1 What we cover this week 6.2 Readings 6.3 Lab", " Week 6 Hypothesis Testing I This week we introduce hypothesis testing for means. 6.1 What we cover this week Hypothesis testing for means - one sample Hypothesis testing for means - two sample, independent groups 6.2 Readings Chapter XII in online textbook Key sections: Single Mean Difference between 2 means Correlated Pairs Statistical literacy 6.3 Lab This lab is due on the Dropbox on MyLearningSpace on Friday March 12th We are going to explore one of my favourite geospatial technologies for our foray into hypothesis testing, Global Positioning Systems (GPS), which is a specific form of the more general Global Navigation Satellite Systems (GNSS). With the advent of GPS chips in mobile phones, location-aware applications and services are now extremely widely used. The quality of GPS sensors varies considerably and an affect how we use the positioning information we get from them. One particular issue with GPS is that it does not work in doors. If you have even a rough understanding of how GPS works, you will know you have to connect to satellites that receive messages to your receiver, and connecting to multiple satellites allows your exact position on the surface of the earth to be located. If we are inside, we cannot receive signals broadcast from space. Now this issue can get a little more complex when we are using GPS outdoors in areas where we do not have a clear view of the sky. In particular, GPS signals can reflect off of objects in the path between our receiver and the satellite - a phenoemna known as GPS multipath error as the error is due to the multiple paths the signal takes. More advanced GPS receivers use different techniques to minimize this issue. However, multipath error can still be an issue in urban environments with tall buildings nearby or in forested areas with canopy cover. In this lab we will compare the GPS signals we received from two receivers - Garmin GPSmap 62s and an Iphone App on an IPhone 8 Plus phone - the EpiCollect 5. Garmin is considered recreational grade while the IPhone app is not a true GNSS receiver. What we want to explore is the difference in reported positioning for these two receivers. To do a proper error analysis of these units, we would really need to compare them to a third independent benchmark, such as a survey marker or a GPS position obtained from a highly accurate positioning system using differential corrections. Lacking that - we will simply compare the positions to each other, to ask; are the positions statistically different. 6.3.1 Examining with Fake Data We will first create some fake geographic data to examine. One of the great things about working with R is that you can usually find a package to do just about anything you want to do. In this case we want an R function to generate random geographic coordinates. We could for example, use runif which generates random uniform numbers and just set the min and max to -180 and +180 for longitude and -90 and +90 for latitude, but we might want finer control, for example to generate random points within some distance of a known location, or within the bounds of a country or neighbourhood. #install.packages(&quot;randgeo&quot;) This line above only ever needs to be run one time - this installs the package on your computer. You need to load the package in order to use it: library(&quot;randgeo&quot;) We can see how to work with this package by looking at its documentation website. If you do you will see that the function we want is called rg_position which we can get help for to see its arguments: ?rg_position which you will see takes one argument for the number of points you want and another for a bbox - or a bounding box within which points should be generated. We will use the bbox argument to generate some random points within a specific area. Luckily there is a website that makes this process very easy from which you can copy the bounding box coordinates. Given that we’re probably all desparate to travel at this point, I am going to go to one of my favorite places I have travelled to, Arugum Bay in Sri Lanka. Figure 6.1: Arugam Bay, Sri Lanka. Feel free to change this somewhere you want to go! For my study area, we can see the bounding box is 81.808421,6.825655,81.851207,6.852244 and this is reported in Longitude, Latitude pairs for the box, and when we look at the rg_position help it tells it that it requires this information as numeric vector of the form west (long), south (lat), east (long), north (lat). Lets try it rg_position(count = 1, bbox = c(81.808421,6.825655,81.851207,6.852244)) ## [[1]] ## [1] 81.837524 6.837419 Try plugging the result into google maps to see if it lands in the correct location (Google Maps takes latitude/longitude in the search box so you’ll have to switch the order). Figure 6.2: Bounding box coordinates plotted on Google Maps We can get more points by increasing the count argument as follows: rg_position(count = 5, bbox = c(81.808421,6.825655,81.851207,6.852244)) ## [[1]] ## [1] 81.820510 6.836621 ## ## [[2]] ## [1] 81.812479 6.848507 ## ## [[3]] ## [1] 81.839422 6.836808 ## ## [[4]] ## [1] 81.820202 6.829456 ## ## [[5]] ## [1] 81.841141 6.844486 now the rg_position function returns a list of pairs of longitude/latitude pairs. We can convert them to a data frame with a little r magic - set.seed(123) df1 &lt;- data.frame(matrix(unlist(rg_position(count = 30, bbox = c(81.808421,6.825655,81.851207,6.852244))), ncol = 2, byrow = TRUE)) df2 &lt;- data.frame(matrix(unlist(rg_position(count = 30, bbox = c(81.808421,6.825655,81.851207,6.852244))), ncol = 2, byrow = TRUE)) so now we have two data frames each with 30 geographic cooridnates in our area of interest. head(df1) ## X1 X2 ## 1 81.82073 6.846615 ## 2 81.82592 6.849133 ## 3 81.84866 6.826866 ## 4 81.83102 6.849383 ## 5 81.83201 6.837796 ## 6 81.84936 6.837709 head(df2) ## X1 X2 ## 1 81.83688 6.828177 ## 2 81.82485 6.832950 ## 3 81.84328 6.837580 ## 4 81.84308 6.847256 ## 5 81.84241 6.837350 ## 6 81.84070 6.842385 Now, back to GPS and hypothesis testing. We will pretend df1 are coordinates from one GPS and df2 are coordinates from another GPS. Since these were just randomly created we won’t expect the coordinates to be close - but the idea of comparing coordinates will be the same. 6.3.2 One-Sample Hypothesis Testing First we will test the hypothesis of whether the difference is equal to zero. \\(H_0: \\mu = 0\\) \\(H_A: \\mu \\ne 0\\) where \\(\\mu\\) is the population mean difference. To start we have to compute our variable analysis. Given that we have a two-dimensional measure of location that we want to summarize with a single vector, we will convert the geographic coordinates to distance using pythagorous theorum which in r is just implementing \\(d = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}\\) as follows d &lt;- sqrt((df1$X1 - df2$X1)^2 + (df1$X2 - df2$X2)^2) so now we are testing \\(H_0: \\mu = 0\\) on the sample mean difference in d. The function for doing a t-test in r is t.test. t.test(d, mu=0) ## ## One Sample t-test ## ## data: d ## t = 11.851, df = 29, p-value = 1.227e-12 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.01592541 0.02256879 ## sample estimates: ## mean of x ## 0.0192471 As per lecture slides the way we calculate a t-statistic for our vector d is: \\(t = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}}\\) where \\(\\sigma_{\\bar{x}}\\) is \\(\\frac{s}{\\sqrt{n}}\\) which we can hand-calculcate in r as: mean(d) / (sd(d)/sqrt(30)) ## [1] 11.85081 which can see matches the output of the t.test function for the value of the t. We can evaluate probability as well: pt(q = 11.85081, df = 29) ## [1] 1 now what would happen if we actually wanted to test \\(H_0: \\mu = 0.02\\) \\(H_A: \\mu \\ne 0.02\\) the above would change to (mean(d) - 0.02) / (sd(d)/sqrt(30)) ## [1] -0.4635751 Can you see how our conclusion would change and what this would mean? How would result from pt change? 6.3.3 Two-Sample Hypothesis Testing Lets suppose that the first 14 of our data points were collected on a day with overcast weather, and the remaining 16 points were collected on a clear day. We might want to examine whether there was a statistical difference between the two days. This would be comparing two groups; and we would need a two-sample hypothesis test to evaluate: \\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1 \\ne \\mu_2\\) which is to say, there is not a difference (ie the two means are equal) or there is (ie the two means are not equal). First we need to separate out the two groups. We can do this by subsetting into two new vectors as follows: day1 &lt;- d[1:14] day2 &lt;- d[15:30] then we test using t.test again: t.test(day1, day2) ## ## Welch Two Sample t-test ## ## data: day1 and day2 ## t = -0.94932, df = 24.489, p-value = 0.3517 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.009495550 0.003508026 ## sample estimates: ## mean of x mean of y ## 0.01765043 0.02064419 This result shows there is no statistical difference between the values in d between day 1 and day 2 because the p-value associated with the null hypothesis is p=0.3517 which is far greater than any of the nominal significance levels we use in hypothesis testing. Note that the df is different here and no longer n-1 as in the case with single sample hypothesis testing. 6.3.4 GPS Dataset I collected some GPS data using two method described above, the EpiCollect App for IPhone and a Garmin GPS Receiver. Below is a video showing some details of the data collection process, as well as a lengthy illustration of how we bring the field data into R. This may only be of interest to some students. The data can be accessed as follows: df &lt;- read.csv(&quot;https://www.dropbox.com/s/f7xcibye2epgqgy/dfprocessed.csv?dl=1&quot;) open_dis &lt;- df$dis[which(df$loctype==&quot;Open&quot;)] forest_dis &lt;- df$dis[which(df$loctype==&quot;Forest&quot;)] mean(open_dis) ## [1] 24.51924 mean(forest_dis) ## [1] 30.33474 6.3.5 Lab Assignment Generate random points for a study area of interest to you (as in the example above). Conduct a hypotheses test that the mean difference is a) equal to zero then b) greater than 0.02. For each use the t.test function as well as the hand-calculated method shown in the instructions above to confirm your answer. In your answer include your hypothesis statements, p-value, and a sentence interepting your results from each hypothesis test. Include a screenshot showing your bounding box coordinates plotted on Google Maps (out of 5) For the GPS data, is the mean difference significantly different from zero (\\(\\alpha = 0.05\\))? Use the t.test function as well as the hand-calculated method shown in the instructions above to confirm your answer. In your answer include your hypothesis statements, p-value, and a sentence interepting your results from each hypothesis test. (out of 5) For the GPS data, was the difference between the two receivers statistically different between locations under forest canopy vs .open sky? Was there more or less difference when under forest canopy? Conduct both one tailed and two tailed hypothesis tests. For this question you can just use the t.test function. In your answer include your hypothesis statements, p-value, and a sentence interepting your results from each hypothesis test. (out of 5) For the GPS data, test whether the distance between the two receivers were statistically lower when accuracy was low (&lt;=12, vs high &gt;12). For this question you can just use the t.test function. In your answer include your hypothesis statements, p-value, and a sentence interepting your results from each hypothesis test. (out of 5) "],
["anova.html", "Week 7 ANOVA 7.1 What we cover this week 7.2 Readings 7.3 Lab", " Week 7 ANOVA This week we introduce the analysis of variance method for comparing multiple group means. 7.1 What we cover this week Hypothesis testing for more than two means 7.2 Readings Chapter XV in online textbook Key sections: all 7.3 Lab This lab is due on the Dropbox on MyLearningSpace on Friday March 19th Someone suggested in lecture we analyze snowfall data, so let’s do it. There are many places to pull weather/climate data online, most of which draw their source data from Environment Canada weather station network. There is an r package to pull data directly into r from the Environment Canada API, but we will use another site weatherstats.ca which has a nice dasboard interface onto the EC data. However if we want to do some statistical analysis ourselves we need to bring this data intor. We will compare snowfall in three Ontario cities, Hamilton, Toronto, and Ottawa. I have done the heavy lifting in taking data from https://ottawa.weatherstats.ca/, https://toronto.weatherstats.ca/, and https://hamilton.weatherstats.ca/ and formatted it into a nice data.frame object which you can read in as follows: x &lt;- read.csv(&quot;https://www.dropbox.com/s/ze5saukvq1tjzbs/snow.csv?dl=1&quot;) Now let’s take a look at this dataset: names(x) #check out the column names to see what variables we have ## [1] &quot;Year&quot; &quot;Snow&quot; &quot;City&quot; head(x) #check out first few rows ## Year Snow City ## 1 1996 122.4 cm Toronto ## 2 1997 143.6 cm Toronto ## 3 1998 61.5 cm Toronto ## 4 1999 117.8 cm Toronto ## 5 2000 135.7 cm Toronto ## 6 2001 81.6 cm Toronto tail(x) #check out last few rows ## Year Snow City ## 95 2015 176.8 cm Ottawa ## 96 2016 312.9 cm Ottawa ## 97 2017 258.4 cm Ottawa ## 98 2018 247.4 cm Ottawa ## 99 2019 269.7 cm Ottawa ## 100 2020 190.6 cm Ottawa nrow(x) #see total number of rows in the dataset ## [1] 100 Well we can see that there are 100 rows, where each row corresponds to year annual total snowfall in a particular city. Let’s checkout what cities/years of data are here: unique(x$City) ## [1] &quot;Toronto&quot; &quot;Hamilton&quot; &quot;Montreal&quot; &quot;Ottawa&quot; summary(x$Year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1996 2002 2008 2008 2014 2020 So we have four cities, and data ranging from 1996 to 2020. Our variable of interest here is snowfall, which is recorded in centimeters. Unfortunately, we have the units ‘cm’ stored as data - something which happens a lot but can make analysis more complicated. As it stands now, r has no way of knowing that the values in the Snow column are numeric, so the values also has text. We can verify this by checking the class of the column class(x$Snow) ## [1] &quot;character&quot; which means it is character (i.e., text). We need to just pull out the numeric part of the values in the Snow column so that we can analyze them statistically. If you try to do analysis of the data now you will get an error: mean(x$Snow) ## [1] NA so we look at the values and look for a pattern for what exactly we want: head(x$Snow, 10) ## [1] &quot; 122.4 cm &quot; &quot; 143.6 cm &quot; &quot; 61.5 cm &quot; &quot; 117.8 cm &quot; &quot; 135.7 cm &quot; ## [6] &quot; 81.6 cm &quot; &quot; 114.9 cm &quot; &quot; 129.6 cm &quot; &quot; 134.9 cm &quot; &quot; 162.6 cm &quot; there are a few of things to take notice of. The numbers part of the values in x$Snow are surrounded by blank spaces. Also, there is an empty space after the “cm” part. What we need to do is a little string parsing - much of learning r or any other technical skill often boils down to knowing what to Google. The best library for doing this kind of thing in r is called stringr - which you will have to install with install.packages('stringr') before you can load it. Let’s formulate a set of rules for exactly how we want to pull out the numeric part of the values in the x$Snow data. get rid of trailing spaces pull out everything up to the first space convert output of 2 to a numeric variable This is really easy to do in r but would be tedious and annoying to do by hand. We will wrap our examples in the head function so we just see the first few rows of output library(stringr) head(str_trim(x$Snow)) #get rid of trailing spaces using str_trim ## [1] &quot;122.4 cm&quot; &quot;143.6 cm&quot; &quot;61.5 cm&quot; &quot;117.8 cm&quot; &quot;135.7 cm&quot; &quot;81.6 cm&quot; #now a little stringr magic: x$snowcm &lt;- str_trim(str_sub(str_trim(x$Snow), start = 1, end = str_locate(str_trim(x$Snow), &quot; &quot;)[,1])) class(x$snowcm) ## [1] &quot;character&quot; which shows we’re almost there, we just have to convert it to numeric in r which we can do now with as.numeric as follows: x$snowcm &lt;- as.numeric(x$snowcm) summary(x$snowcm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 32.4 122.4 166.3 171.9 203.5 374.1 hist(x$snowcm) OK so we now have total snowfall by city by year, which we can use to do our analysis. 7.3.1 Analysis of Variance in R We have just looked at doing a hypothesis test for a difference between two means. But what if we want to test for differences across three means? Or five? This is very common – when you have a categorical demographic variable, and you want to compare a numeric variable across the categories. We can set up a hypothesis test for multiple groups by comparing the variation between and within groups of data. This approach is called Analysis Of Variance (ANOVA). For ANOVA, we use a test statistic that is a ratio of sums of squares, and it follows an F distribution and is therefore called an F statistic. The F statistic can be calculated for ANOVA as: \\(F=\\frac{MSB}{MSE}\\) Where MSB is the mean squares between groups, and MSE is the mean squares within groups. Refer to lecture notes for full description of these terms. 7.3.1.1 Mean Squares Between (MSB) We will focus now on the MSB part in the equation above. This is the ‘mean squares between’ - which quantifies the amount of ‘between group differences’. If this is high the F goes above 1 and we have evidence of between group differences in means. As noted in lecture, the MSB is \\(MSB = \\frac{BSS}{k-1}\\) where BSS is between group sums of squares and k is the number of groups being compared. \\(BSS = \\sum{n_i}(\\bar{x_i}-\\bar{\\bar{x}})^2\\) where \\(\\bar{x}_i\\) is the mean from the ‘i’th’ group, and \\(\\bar{\\bar{x}}\\) is the grand mean. Let’s think about how to calculate these pieces. The \\(\\bar{x}_i\\) are easy enough they are the sample means. Let’s examine on the golf club data below (use ANOVA lecture slides to compare): c1 &lt;- c(254,263,241,237,251) #sample data 1 c2 &lt;- c(234,218,235,227,216) #sample data 2 c3 &lt;- c(200,222,197,206,204) #sample data 3 xb1 &lt;- mean(c1) #sample mean 1 xb2 &lt;- mean(c2) #sample mean 2 xb3 &lt;- mean(c3) #sample mean 3 gm &lt;- mean(c(xb1, xb2, xb3)) #grand mean Therefore BSS becomes: BSS &lt;- 5 * (xb1-gm)^2 + 5 * (xb2-gm)^2 + 5 * (xb3-gm)^2 BSS ## [1] 4716.4 7.3.1.2 Mean Squares Error (MSE) And ESS is the within-group sum of squares (or error sum of squares) which is \\(ESS = \\sum_{i=1}^k{}\\sum_{j=1}^n{(x_{ij}-\\bar{x_j})^2}\\) or ESS &lt;- sum((c1-xb1)^2) + sum((c2-xb2)^2) + sum((c3-xb3)^2) ESS ## [1] 1119.6 therefore: MSB &lt;- BSS/2 MSE &lt;- ESS/12 Fstat &lt;- MSB/MSE Fstat ## [1] 25.27546 then get the appropriate F-crit qf(.05, df1=2, df2=12, lower.tail = FALSE) ## [1] 3.885294 and make our conclusion. For any model, we do get the F statistic hypothesis test as the last line in the output. The hypothesis being tested is that the F is equal to 1, the alternate is that it is not equal to one. The F distribution is used to characterize the f statistic (surprise). Because this is a ratio of positive numbers (both are sums of squares), the F takes on only positive numbers. The shape of the F statistic is as below. fdata3 &lt;- df(seq(0,5,.01), df1=4, df2=3) fdata2 &lt;- df(seq(0,5,.01), df1=5, df2=5) fdata1 &lt;- df(seq(0,5,.01), df1=6, df2=10) plot(seq(0,5,.01), fdata1, type=&quot;l&quot;, ylab = &quot;Probability density&quot;, xlab=&quot;F-Statistic&quot;) lines(seq(0,5,.01), fdata2, type=&quot;l&quot;, col=&quot;red&quot;) lines(seq(0,5,.01), fdata3, type=&quot;l&quot;, col=&quot;blue&quot;) These are just some examples of possible f distributions. As with any hypothesis test, we compare the observed value of an F statistic to a critical value from the distribution, for a given alpha (significance level). Or we just compare the p-value to our alpha. Key things to note about the F distribution, we now have two degrees of freedom, one for the numerator and one for the denominator. In the end you don’t have to know too much detail about it because the way we use it is the same as the normal and t distributions, we compute a value of the test statistic for our data and compare it a sampling distribution associated with the null hypothesis. We then make a conclusion about our hypothesis based on this comparison (i.e., reject or fail to reject). Finally, if our data are formatted as a table, we can use the built in anova function to compare means ranges &lt;- c(c1, c2, c3) clubs &lt;- c(rep(&quot;C1&quot;, 5), rep(&quot;C2&quot;, 5), rep(&quot;C3&quot;, 5)) df &lt;- data.frame(ranges, clubs) anova(lm(ranges~clubs, data=df)) ## Analysis of Variance Table ## ## Response: ranges ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## clubs 2 4716.4 2358.2 25.276 4.985e-05 *** ## Residuals 12 1119.6 93.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3.2 Lab Assignment Use ANOVA to compare whether snowfall differs among the four cities in the dataset. State your hypothesis, test statistics, p-value and a sentence interpreting your result. Can use built in function or manual method - either are OK. (out of 5) Use ANOVA to compare whether snowfall differs among (any) three of the four cities in the dataset. State your hypothesis, test statistics, p-value and a sentence interpreting your result. Can use built in function or manual method - either are OK. (out of 5) Conduct some analysis of your choosing on the snow dataset to explore a question you find interesting. You can incorporate external data if you want. You can use any approach you want including but not limited to hypothesis testing, graphs, confidence intervals etc. Write up a paragraph interepreting your analysis - discuss any limitations (out of 10) One of the foundational assumptions of classical statistical inference is that observations are indepenent. Can you think of any potential violations to this assumption with the snow dataset? (out of 5) Bonus question: Explain in detail (i.e., what each function is doing) what is happening in this step below #now a little stringr magic: x$snowcm &lt;- str_trim(str_sub(str_trim(x$Snow), start = 1, end = str_locate(str_trim(x$Snow), &quot; &quot;)[,1])) "],
["linear-regression.html", "Week 8 Linear Regression 8.1 What we cover this week 8.2 Readings 8.3 Lab", " Week 8 Linear Regression This week we introduce linear regression and the various statistical inference tools used in linear regression analysis. 8.1 What we cover this week Linear regression concepts Ordinary least squares Hypothesis testing and regression 8.2 Readings Chapter XV in online textbook Key sections: Introduction to simple linear regression Partitioning sums of squares Standard error of the estimate Inferential statistics for b and r 8.3 Lab This lab is due on the Dropbox on MyLearningSpace on Friday April 9th For this lab we will use one of the built in datasets. R has a number of sample datasets and many packages also have sample data that comes with them. We will use the mtcars dataset that includes data from a 1974 Motor Trend magazine describing characteristics of 32 cars. You can check out the details of the variables by examining the documentation with ?mtcars The mtcars dataset is a data frame. This is a tabular data structure, the individual columns of the data frame can be accessed using the $ notation. For example: summary(mtcars$mpg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.40 15.43 19.20 20.09 22.80 33.90 We will use this dataset to explore linear regression. The function for fitting a linear regression model is lm. We can create a linear regression object by assigning the output of lm to a new variable, or we wrap it in a summary command. Here is an example summary(lm(mtcars$mpg~mtcars$wt)) ## ## Call: ## lm(formula = mtcars$mpg ~ mtcars$wt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## mtcars$wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 Another way to do the same thing is to just specify the column names but then set the data argument in the lm function to specify the name of the data frame that the columns are in summary(lm(mpg~wt, data=mtcars)) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 Examining the description of the columns with ?mtcars we can see that this is creating a linear regression model with Miles/(US) gallon as the dependent variable and Weight (1000 lbs) as the independent variable. Thinking about this, and the fact these are cars from the early 1970s, we might expect the relationship between these two variables to be negative - as car weight increases the miles/per gallon should decrease. Looking at the output above, we see our two coefficient estimates are in the “Estimate” column. The “(Intercept)” Estimate is 37.2851 and the slope coefficient for wt is -5.3445. We interpret the Y-intercept as the value of Y when the X variable is zero. Often, the Y-intercept does not have any real meaning. The slope coefficient is usually what we are interested in. Here a value of -5.3445 means the average change in Miles/(US) gallon for a one-unit increase in weight. It is important to think about what the units the data are measured in. We can see from the documentation that the wt variable is measured in 1000s of lbs. Therefore for every extra 1000lbs of car weight, the miles per gallon decreases by -5.3445. The next column in the regression output is the standard error column. These are standard errors on the coeffient estimates, and next to that is the t-statistics. Recall that these are t-test on if the coefficents are significant, i.e., are they statistically different from zero. \\(t = \\frac{\\beta_1 - 0}{\\beta_{1se}}\\) so for the wt variable, the t-statistic is just \\(t=\\frac{-5.3445 - 0}{0.5591}=-9.559\\) and the last column is the p-value on the hypothesis test. The Residual standard error is the standard error of the estimate. We get the R-squared as well which we can calculate from the SSR/SST formulas. Or we can see the sums of squares by using the anova function such as anova(lm(mpg~wt, data=mtcars)) ## Analysis of Variance Table ## ## Response: mpg ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wt 1 847.73 847.73 91.375 1.294e-10 *** ## Residuals 30 278.32 9.28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 where SSR=847.73, SSE=278.32, and SST = SSR+SSE, so \\(R^2=\\frac{SSR}{SST}\\) - which expresses the percentage of variation in the Y variable explained by the X variable. Here we see that \\(R^2=\\frac{847.73}{1126.05}=0.7528\\). We also see the F statistic which is a hypothesis test on the model as a whole, and just as in our anova for comparing means, we are using the SSR/SST as the basis for the F statistic. We often want to get the model residuals to examine their distribution. We can do this by using the residuals function err &lt;- residuals(lm(mpg~wt, data=mtcars)) hist(err) which we can see are centered on zero and approximately normally distributed. We can also check the heterogeneity by comparing the residuals against the x values plot(mtcars$wt, err) we can also see some of these diagnostic plots by creating an lm object by assigning the output of lm to a new object and then plotting it. Read up on how to interpret these diagnostics here lmo &lt;- lm(mpg~wt, data=mtcars) plot(lmo) 8.3.1 Lab Assignment Create a regression model using Miles/(US) gallon as the dependent variable and Gross horsepower as the independent variable. Report all model statistics and comment on the model diagnostics. Include a basic scatterplot of the data as well. Write a sentence or two interpreting this analysis. (out of 5) Create a regression model using two other variables from the mtcars dataset (i.e., not miles per gallon as dependent and horsepower as independent). Ensure that the there is a plausible causal connection between the independent and dependent variables. Report all model statistics and comment on the model diagnostics. Include a basic scatterplot of the data as well. Write a sentence or two interpreting this analysis. (out of 5) What would be the problem with using all of the variables (other than mpg) in the mtcars dataset as independent variables to predict mpg? (out of 5) Look at the other datasets available in the base package by checking library(help = \"datasets\"). Pick one and create a linear regression model. Report all model statistics and comment on the model diagnostics. Include a basic scatterplot of the data as well. Write a sentence or two interpreting this analysis. (out of 5) Using the quakes dataset, examine whether there is a significant relationship between earth quake depth and earthquake magnitude. Report all relevant statistics and interpret the findings. (out of 5) Using everything you now know how to do, explore a research question of interest in detail, using either your own external dataset or one of the ones used previously in lab or the built in datasets. In your analysis, do a confidence interval, at least three graphs, one hypothesis test, and one regression analysis. Try to put your analysis together into a logical flow and write a paragraph or two interpreting your findings. (out of 15) "]
]
